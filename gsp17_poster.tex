%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% baposter Landscape Poster
% LaTeX Template
% Version 1.0 (11/06/13)
%
% baposter Class Created by:
% Brian Amberg (baposter@brian-amberg.de)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[b1paper,fontscale=0.325]{baposter} % Adjust the font scale/size here

\usepackage[english]{babel}
\usepackage{graphicx} % Required for including images
\graphicspath{{figures/}} % Directory in which figures are stored

\usepackage{amsmath} % For typesetting math
\usepackage{amssymb} % Adds new symbols to be used in math mode
\usepackage{braket}  % For Bra-Ket notation

\usepackage{booktabs} % Top and bottom rules for tables
\usepackage{enumitem} % Used to reduce itemize/enumerate spacing
\usepackage{palatino} % Use the Palatino font
\usepackage[font=small,labelfont= bf]{caption} % Required for specifying captions to tables and figures

\usepackage{multicol} % Required for multiple columns
\setlength{\columnsep}{1.5em} % Slightly increase the space between columns
\setlength{\columnseprule}{0mm} % No horizontal rule between columns

\usepackage{natbib}
\setlength{\bibsep}{0.0pt}

\usepackage{algorithmic}
\usepackage{caption}
\captionsetup[figure]{labelformat=empty}
\usepackage{subcaption}
\usepackage[ruled]{algorithm2e}
\usepackage{color}

%\usepackage{tikz} % Required for flow chart
%\usetikzlibrary{shapes,arrows} % Tikz libraries required for the flow chart in the template

% Example definitions.
% --------------------
\newcommand{\Abf}{\mbox{${\bf A }$} }
\newcommand{\Bbf}{\mbox{${\bf B }$} }
\newcommand{\Dbf}{\mbox{${\bf D }$} }
\newcommand{\Ibf}{\mbox{${\bf I }$} }
\newcommand{\Kbf}{\mbox{${\bf K }$} }
\newcommand{\Lbf}{\mbox{${\bf L }$} }
\newcommand{\Mbf}{\mbox{${\bf M }$} }
\newcommand{\Pbf}{\mbox{${\bf P }$} }
\newcommand{\Qbf}{\mbox{${\bf Q }$} }
\newcommand{\Sbf}{\mbox{${\bf S }$} }
\newcommand{\Ubf}{\mbox{${\bf U }$} }
\newcommand{\Vbf}{\mbox{${\bf V }$} }
\newcommand{\Wbf}{\mbox{${\bf W }$} }
\newcommand{\Xbf}{\mbox{${\bf X }$} }
\newcommand{\Ybf}{\mbox{${\bf Y }$} }
\newcommand{\abf}{\mbox{${\bf a }$} }
\newcommand{\bbf}{\mbox{${\bf b }$} }
\newcommand{\dbf}{\mbox{${\bf d }$} }
\newcommand{\ebf}{\mbox{${\bf e }$} }
\newcommand{\kbf}{\mbox{${\bf k }$} }
\newcommand{\pbf}{\mbox{${\bf p }$} }
\newcommand{\qbf}{\mbox{${\bf q }$} }
\newcommand{\ubf}{\mbox{${\bf u }$} }
\newcommand{\vbf}{\mbox{${\bf v }$} }
\newcommand{\wbf}{\mbox{${\bf w }$} }
\newcommand{\xbf}{\mbox{${\bf x }$} }
\newcommand{\ybf}{\mbox{${\bf y }$} }
\newcommand{\Ecal}{\mbox{${\cal E }$} }
\newcommand{\Gcal}{\mbox{${\cal G }$} }
\newcommand{\Kcal}{\mbox{${\cal K }$} }
\newcommand{\Lcal}{\mbox{${\cal L }$} }
\newcommand{\Vcal}{\mbox{${\cal V }$} }

\input{macros}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% % This is very inflexible
%\newcommand{\compresslist}{ % Define a command to reduce spacing within itemize/enumerate environments, this is used right after \begin{itemize} or \begin{enumerate}
%\SetEnumitemValue{\leftmargin}{!}
%\setlength{\itemsep}{1pt}
%\setlength{\topsep}{1pt}
%\setlength{\parsep}{0pt}
%}

% Key for itemize and enumerate to reduce spacing
\SetEnumitemKey{compresslist}{
labelindent=0pt,labelwidth=1em,leftmargin=!,topsep=1pt,itemsep=4pt,parsep=0pt
}

% Custom colors
\definecolor{cardinal}{cmyk}{0,1,0.50,0.51}

\begin{document}

\begin{poster}
{
columns=2,
colspacing=0.5em,
headerborder=closed, % Adds a border around the header of content boxes
colspacing=1em, % Column spacing
bgColorOne=white, % Background color for the gradient on the left side of the poster
bgColorTwo=white, % Background color for the gradient on the right side of the poster
borderColor=cardinal, % Border color
headerColorOne=white, % Background color for the header in the content boxes (left side)
headerColorTwo=white, % Background color for the header in the content boxes (right side)
headerFontColor=cardinal, % Text color for the header text in the content boxes
boxColorOne=white, % Background color of the content boxes
textborder=roundedleft, % Format of the border around content boxes, can be: none, bars, coils, triangles, rectangle, rounded, roundedsmall, roundedright or faded
eyecatcher=false, % Set to false for ignoring the left logo in the title and move the title left
headerheight=0.1\textheight, % Height of the header
headershape=roundedright, % Specify the rounded corner in the content box headers, can be: rectangle, small-rounded, roundedright, roundedleft or rounded
headerfont=\large\bf\textsf, % Large, bold and sans serif font in the headers of content boxes
%textfont={\setlength{\parindent}{1.5em}}, % Uncomment for paragraph indentation
linewidth=1pt, % Width of the border lines around content boxes
textfont=\textsf
}
%----------------------------------------------------------------------------------------
%	TITLE SECTION 
%----------------------------------------------------------------------------------------
%
{\includegraphics[height=3em]{logo_MERL.pdf}} % First university/lab logo on the left
{\huge \bfseries \textcolor{cardinal}{Disc-GLasso: Discriminative Graph Learning \\with Sparsity Regularization} \vspace{0.3em}} % Poster title
{\large \textbf{Jiun-Yu Kao$^{1}$, Dong Tian$^{2}$, Hassan Mansour$^{2}$, Antonio Ortega$^{1}$ and Anthony Vetro$^{2}$}  
\\ \textcolor{gray}{\small{$^{1}$ University of Southern California, Los Angeles, CA}} 
\\ \textcolor{gray}{\small{$^{2}$ Mitsubishi Electric Research Labs (MERL), Cambridge, MA}}
%\\ \textcolor{gray}{\small{\{agadde, aanis\}@usc.edu, ortega@sipi.usc.edu}} 
} % Author names and institution
{\includegraphics[height=7.5em]{logo_usc_merl.pdf}} % Second university/lab logo on the right

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\headerbox{Motivation}{name=motiv,column=0,row=0}{

Graph structures are natural to use 
\begin{itemize}[compresslist]
\item Construct optimal graph not trivial
	\begin{itemize}[compresslist]
	\item benefit subsequent data analysis \& could learn from data
	\end{itemize}
\item Prior art learn graphs w.r.t. 
	\begin{itemize}[compresslist]
	\item Representability: benefits energy compaction
	\item Sparsity: benefits interpretation
	\end{itemize}
\end{itemize} 

{\textbf{Objectives}}
\begin{itemize}[compresslist]
\item Multi-class classification among data samples (graph signals)
\item Develop approach to learn a set of graphs that benefits classification
\end{itemize}

%{\textbf{Prior Art Approach and Issue}}\\
%Apply GLasso for data in each class independently
%\begin{itemize}[compresslist]
%	\item Only consider representability and sparsity within each class
%	\item Discrimination between classes not guaranteed
%\end{itemize}

{\textbf{Key Contributions}}
\begin{itemize}[compresslist]
\item First propose multi-graph learning that promotes discrimination
\item Develop efficient algorithm to learn discriminative class-specific graphs 
\end{itemize}

%\vspace{0.1em} % When there are two boxes, some whitespace may need to be added if the one on the right has more content
}

%----------------------------------------------------------------------------------------
%	KEY CONTRIBUTIONS
%----------------------------------------------------------------------------------------

%\headerbox{Key Contributions}{name=key_contrib,column=0,below=motiv}{
%\begin{itemize}[compresslist]
%\item First propose multi-graph learning that promotes discrimination
%\item Develop efficient algorithm to learn discriminative class-specific graphs 
%\end{itemize}
%}

%----------------------------------------------------------------------------------------
%	PROBLEM FORMULATION
%----------------------------------------------------------------------------------------

\headerbox{Problem Formulation}{name=problem,column=0,below=motiv}{

\textbf{Notations}
\begin{itemize}[compresslist]
\item random graph signals in $i$-th class: $\xbf^{(i)}\in \mathbb{R}^{n}$
\item $S$ classes in total
\item For $i$-th class, $N_i$ i.i.d. realizations/samples: $\xbf_1^{(i)},\dots,\xbf_{N_i}^{(i)}$
\item $\Kbf_i$: empirical cov. matrix of samples in $i$-th class
\end{itemize}
\vspace{.3em}
Define \textcolor{red}{multi-category graph learning} problem:\\
\textbf{Goal:} learn graph structure of each category, $\mathcal{G}_1,\dots,\mathcal{G}_S$
\begin{itemize}[compresslist]
\item $\mathcal{G}_i=(\mathcal{V},\mathcal{E}_i,\Qbf_i)$, n-vertex set $\mathcal{V}$, edge set $\mathcal{E}_i$
\item Symmetric matrix representation $\Qbf_i$, where $\forall a\neq b$, $(a,b)\in\mathcal{E}_i \iff \Qbf_{i,ab}\neq 0$
\item $\Qbf_i$ is only required to be positive semi-definite
\end{itemize}

}

%----------------------------------------------------------------------------------------
%	BASELINE
%----------------------------------------------------------------------------------------

\headerbox{Baseline Approach}{name=baseline,column=0,below=problem}{

Apply graphical lasso indep. to each class:
\begin{itemize}[compresslist]
\item Solve $\ell_1$-penalized Gaussian ML estimation problem separately for the graph of each category $i$, 
			$$\small \mathrm{min} -\log\det(\Qbf_i) + \mathrm{tr}(\Kbf_i\Qbf_i) + \rho {\|\Qbf_i\|}_1$$
\vspace{-1.7em}
	\begin{itemize}[compresslist]
	\item $\mathrm{tr}(\Kbf_i\Qbf_i)=\frac{1}{N_i} \sum_{k=1}^{N_i} {\xbf_k^{(i)}}^T \Qbf_i \xbf_k^{(i)}$
	\item Minimize above term $\Rightarrow$ promote smoothness of data samples in $i$-th class on $i$-th graph
	\end{itemize}
\item Only favor energy compaction and sparsity within each class %$\Rightarrow$ not guarantee to be effective in discriminating between classes
\item Discrimination between classes not guaranteed
\end{itemize}

}

%----------------------------------------------------------------------------------------
%	DISC-GLASSO ALGORITHM
%----------------------------------------------------------------------------------------

\headerbox{Disc-GLasso Algorithm}{name=disc_glasso,column=0,below=baseline,above=bottom}{

For each $\Wbf_i$, given the empirical covariance matrices $\Kbf_1,\cdots,\Kbf_S$.
\begin{enumerate}[compresslist]
\item Search for the minimum ratio $r$ such that $\Kbf_i - \frac{1}{r} \sum_{j\neq i}^S \Kbf_j \succeq 0, \forall i$.
%\vspace{.1em}			
\item Initialize with $\Wbf_i = \Kbf_i + \rho\Ibf - \frac{1}{r}\sum_{j\neq i}^S \Kbf_j$. The diagonal of $\Wbf_i$ will remain unchanged in what follows.
\vspace{.1em}
\item Perform following steps until convergence reached:
\vspace{.1em}
	\begin{enumerate}[compresslist]
	\item Rearrange the rows/columns so that the target column is last.
	%\vspace{.1em}
	\item Solve the lasso problem for $\hat{\beta}$:
	\vspace{-.5em}
	\footnotesize	
	$$\mathrm{arg}\underset{\beta}{\mathrm{min}} \frac{1}{2} {\left\| {\Wbf_{11}^i}^{1/2}\beta - {\Wbf_{11}^i}^{-1/2}\kbf_{12}^i + \frac{1}{r}\sum_{j\neq i}^S {\Wbf_{11}^i}^{-1/2}\kbf_{12}^j \right\|}^2 + \rho {\|\beta\|}_1$$
	\vspace{-1em}
	\normalsize
	\item Fill in the corresponding row and column of $\Wbf_i$ using $\wbf_{12}^i = \Wbf_{11}^i \hat{\beta}$.
	\end{enumerate}
\end{enumerate}

}

%----------------------------------------------------------------------------------------
%	PROPOSED SOLUTION
%----------------------------------------------------------------------------------------

\headerbox{Proposed Solution}{name=solution,column=1,row=0}{

\begin{itemize}[compresslist]
\item Key idea: jointly learn the graphs for all classes by promoting
	\begin{itemize}[compresslist]
    \item Smoothness of data in $i$-th class on $i$-th graph
    \item \textcolor{red}{Non-smoothness on learned graphs corresponding to the other classes}
    \end{itemize}
\item To achieve above properties, we need to minimize the following, %on $\Qbf_1,\dots,\Qbf_S$
\vspace{-.5em}
	\small
	\begin{equation*}			
	\sum_{i=1}^S \sum_{k=1}^{N_i} \frac{1}{N_i} \left[ 
	{\xbf_k^{(i)}}^T\Qbf_i\xbf_k^{(i)} - \frac{1}{S-1}\sum_{j\neq i}^S {\xbf_k^{(i)}}^T \Qbf_j \xbf_k^{(i)}\right]
	\end{equation*}
	\normalsize
\vspace{-1.5em}
\item Finally, we propose to solve the optimization problem below.
\vspace{-1em}
\small
\begin{equation}
\label{eq:4}
\small
\underset{\Qbf_i\succeq 0}{\mathrm{min}} -\log\det(\Qbf_i) + \mathrm{tr}(\Kbf_i\Qbf_i) \textcolor{red}{ \,-\, \frac{\mu_i}{S-1}\sum_{j\neq i}^S \mathrm{tr}(\Kbf_j\Qbf_i)} + \rho_i {\|\Qbf_i\|}_1,
\vspace{-.9em}
\end{equation}
\normalsize
for each $\Qbf_i$, given $\Kbf_1, \dots, \Kbf_S$.		
\item A block coordinate descent based algorithm is developed to solve (\ref{eq:4}).  
\end{itemize}
\vspace{.3em}
\textbf{Parameter Choice: }Let $r = \frac{S-1}{\mu}$,
\vspace{.4em}
\begin{itemize}[compresslist]
\item Smaller $r$ $\Rightarrow$ better discrimination 
\item Need to ensure $\Kbf_i - \frac{1}{r} \sum_{j\neq i}^S \Kbf_j \succeq 0, \forall i$ to guarantee that after each updating step $t$, $\Wbf_i^{(t)}\succ 0, \forall t$.
\end{itemize}
\vspace{.4em}
In proposed algorithm, we search for the minimum $r$ s.t. $\Kbf_i - \frac{1}{r} \sum_{j\neq i}^S \Kbf_j \succeq 0, \forall i$, via line search through predefined set of values.
\vspace{.4em}
\begin{itemize}[compresslist]
\item the only additional time cost of our algorithm compared to GLasso
\end{itemize}

}

%----------------------------------------------------------------------------------------
%	EXPERIMENTS
%----------------------------------------------------------------------------------------

\headerbox{Experiments}{name=result,column=1,below=solution}{
%
{\textbf{Synthetic data for binary classification}}\\
{
\vspace{-.5em}
\small
\begin{itemize}[compresslist]
\item \textbf{From left to right:} $\mathcal{G}_1$, $\mathcal{G}_2$ learned with GLasso, and $\mathcal{G}_1$, $\mathcal{G}_2$  learned with Disc-GLasso.
\end{itemize}
\vspace{-.8em}
\begin{center}
\includegraphics[width=.2\textwidth]{figures/glasso_g1_w2y_01_new_cropped_embedded.pdf} \hspace{.3em}
\includegraphics[width=.2\textwidth]{figures/glasso_g2_w2y_01_new_cropped_embedded.pdf} \hspace{.3em}
\includegraphics[width=.2\textwidth]{figures/disc_g1_w2y_01_new_cropped_embedded.pdf} \hspace{.3em}
\includegraphics[width=.2\textwidth]{figures/disc_g2_w2y_01_new_cropped_embedded.pdf} %\vspace{.3em}
%\multicolumn{4}{c}{(a) picture 5 of \textit{cars1}} \\
% {\small (f) frame 27 of \textit{cars10} with three motions}
%\caption{\textbf{From left to right:} $\mathcal{G}_1$, $\mathcal{G}_2$ learned with GLasso, and $\mathcal{G}_1$, $\mathcal{G}_2$  learned with Disc-GLasso.}
\end{center}
}
{
\vspace{-.8em}
\small
\begin{itemize}[compresslist]
\item Cumulative spectrum energy of test signals in class 1 \textbf{(left)} and class 2 \textbf{(right)} on the learned graphs.
\end{itemize}
\vspace{-1em}
\begin{center}
\includegraphics[width=.4\textwidth]{figures/test_spectrum_signal1_new_cropped_embedded.pdf} \hspace{.3em}
	% \quad
\includegraphics[width=.4\textwidth]{figures/test_spectrum_signal2_new_cropped_embedded.pdf}
%\caption{Cumulative spectrum energy of test signals in class 1 \textbf{(left)} and class 2 \textbf{(right)} on the learned graphs.}
\end{center}
}
{
%\begin{figure}
\vspace{-1em}
\small
\begin{itemize}[compresslist]
\item Effect of $r$ on the separation measure $s = \frac{\mathrm{tr}(\Kbf_1\Qbf_2)+\mathrm{tr}(\Kbf_2\Qbf_1)}{\mathrm{tr}(\Kbf_1\Qbf_1)+\mathrm{tr}(\Kbf_2\Qbf_2)}$ \textbf{(left)} and classification accuracy \textbf{(right)}.
\end{itemize}
\vspace{-1em}
\begin{center}
\includegraphics[width=.35\textwidth]{s_measure_new_v2_cropped_embedded.pdf} \hspace{.6em}
% \quad
\includegraphics[width=.35\textwidth]{classification_r_v2_cropped_embedded.pdf}
%\caption{The separation measure \textbf{(left)} and classification accuracy \textbf{(right)} versus $r$.}
\end{center}
%\end{figure}
}   

}

%----------------------------------------------------------------------------------------
%	ACTIVE SEMI-SUPERVISED LEARNING - CONTINUED
%----------------------------------------------------------------------------------------

%\headerbox{Active Semi-supervised Learning (continued)}{name=active_learning_cotd,column=2,row=0}{
%
%\vspace{0.1em}
%\hspace{0.2em} {\textbf{Maximizing Cut-off Frequency $\Rightarrow$ Active Learning}}
%%
%\begin{itemize}[compresslist]
%\item Cut-off $\Omega_k(\Sc) \equiv$ variation of smoothest signal $\phi_\text{opt}$ in $L_2(\Scc)$.
%\end{itemize}
%%
%\begin{minipage}{0.6\linewidth}
%\begin{itemize}[compresslist]
%\item Larger cut-off $\Rightarrow$ more variation in $\phi_{\text{opt}} \Rightarrow$ more cross-links.
%\item Unlabeled nodes are strongly connected to labeled nodes!
%\end{itemize} 
%\end{minipage}%
%%
%\hfill
%\begin{minipage}{0.38\linewidth}
%\hfill
%\includegraphics[width=0.48\linewidth]{figures/samp_set_intuition_2.pdf}
%\includegraphics[width=0.48\linewidth]{figures/samp_set_intuition_1.pdf}
%\end{minipage}%
%
%\vspace{1em}
%\hspace{0.5em} {\large\textbf{P3: Label prediction by reconstruction}}
%
%\vspace{0.5em}
%\begin{minipage}{0.72\linewidth}
%%
%\begin{itemize}[compresslist]
%\item $\Cc_1 = \{\xv: \xv(\Sc) = \fv(\Sc)\}$ and $\Cc_2 = PW_{\omega}(G)$.
%\item Solution: $\fv \in \Cc_1 \cap \Cc_2$, sampling theory $\Rightarrow$ unique $\fv$.
%\item POCS: $\fv_{i+1} = \Pm_{\Cc_2}\Pm_{\Cc_1} \fv_i$, where $\fv_0 = [\fv(\Sc)^\top, \zerov]^\top$.
%\begin{itemize}[compresslist]
%\item $\Pm_{\Cc_1}$ resets the samples on $\Sc$ to $\fv(\Sc)$.
%\item $\Pm_{\Cc_2} = \Um h(\mathbf{\Lambda}) \Um^\top$ sets $\tilde{\fv}(\lambda) = 0$ if $\lambda > \omega$. 
%\end{itemize}
%\end{itemize} 
%\end{minipage}%
%%
%\hfill
%\begin{minipage}{0.27\linewidth}
%\begin{center}
%\includegraphics[width=\linewidth]{figures/pocs-1.pdf}
%\end{center}
%\end{minipage}%
%%
%\begin{itemize}[compresslist]
%\item $\Pm_{\Cc_2} \approx \sum_{i=1}^n \left(\sum_{j=0}^p a_j \lambda_i^j\right) \uv_i \uv_i^\top = \sum_{j=0}^p a_j \Lcb^j \rightarrow$ $p$-hop localized
%\end{itemize}
%}

%----------------------------------------------------------------------------------------
%	ACTIVE SEMI-SUPERVISED LEARNING - CONTINUED
%----------------------------------------------------------------------------------------

%\headerbox{Results}{name=results,column=2,below=active_learning_cotd}{
%
%\textbf{Toy example}:
%\begin{center}
%\includegraphics[width=0.92\linewidth]{figures/toy_result.pdf}
%\end{center}
%%
%\textbf{Real datasets}:
%\begin{center}
%\includegraphics[width=0.92\textwidth]{figures/real_data_result.pdf}
%\end{center}
%%
%\textbf{Effect of $k$}:
%\begin{center}
%\includegraphics[width=0.92\textwidth]{figures/effect_of_k.pdf}
%\end{center}
%}

%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

\headerbox{References}{name=references,column=1,below=result,bottomaligned=disc_glasso}{
\renewcommand{\section}[2]{\vskip 0.05em} % Get rid of the default "References" section title
\nocite{*} % Insert publications even if they are not cited in the poster
\small{ % Reduce the font size in this block
%\bibliographystyle{plain}
%\bibliography{references_v2}
\begin{enumerate}[compresslist]
\item J. Friedman et.al, ``Sparse inverse covariance estimation with the graphical lasso,'' Biostatistics, 2008.
\item R. Mazumder and T. Hastie, ``The graphical lasso: New insights and alternatives,'' Elect. journal of stat., 2012.
%\item H. Mansour, et.al, ``Video background subtraction using semi-supervised robust matrix completion,'' IEEE ICASSP, 2014
\end{enumerate}
}
}

%----------------------------------------------------------------------------------------

\end{poster}

\end{document}
